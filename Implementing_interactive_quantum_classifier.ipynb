{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5J1AkiSErOm8"
      },
      "outputs": [],
      "source": [
        "#code of Interactive Quantum Classifier Inspired by Quantum Open System Theory\n",
        "#LINK https://ieeexplore.ieee.org/document/9533917\n",
        "\n",
        "#LINK https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9533917\n",
        "\n",
        "#this code was written by Fernando Maciano de Paula Neto (fernando@cin.ufpe.br) and Eduardo Barreto Brito (ebb2@cin.ufpe.br)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from scipy.linalg import expm as expMatrix\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_sigmaE(vectorX, vectorW):\n",
        "  \"\"\"\n",
        "    Multiplies the input (vectorX) by the weights (vectorW),\n",
        "    resulting in a diagonal matrix.\n",
        "    In case any of the vectors contains imaginary parts, it's discarded.\n",
        "    Equivalent of Equation #17 in the Article.\n",
        "  \"\"\"\n",
        "  n = len(vectorX)\n",
        "  sigmaE = np.zeros((n,n))\n",
        "  for i in range(n):\n",
        "    sigmaE[i,i] = np.real(vectorX[i])*np.real(vectorW[i])\n",
        "\n",
        "  return sigmaE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_sigmaQ(n):\n",
        "  \"\"\"\n",
        "    Sums sigmaX, sigmaY and sigmaZ to get sigmaQ.\n",
        "    - sigmaX comes from Equation #7 = [0, 1   1, 0]\n",
        "    - sigmaY comes from Equation #8 = [0, -i  i, 0]\n",
        "    - sigmaZ comes from Equation #9 = [1, 0   0, -1]\n",
        "    Equivalent of Equation #16 in the Article.\n",
        "  \"\"\"\n",
        "  sigmaQ = np.zeros((n,n))\n",
        "  sigmaX = np.array([[0,1], [1,0]])\n",
        "  sigmaY = np.array([[0,-1j], [1j,0]])\n",
        "  sigmaZ = np.array([[1,0], [0,-1]])\n",
        "  sigmaQ = sigmaX + sigmaY + sigmaZ\n",
        "\n",
        "  return sigmaQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_U_operator(sigmaQ, sigmaE):\n",
        "  \"\"\"\n",
        "  Makes the exponential matrix of tensor product between sigmaQ and sigmaE and multiplies it by j. \n",
        "  Equivalent of Equation #15 in the Article.\n",
        "  \"\"\"\n",
        "  return np.matrix(expMatrix(1j*np.kron(sigmaQ, sigmaE)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_p(psi):\n",
        "  \"\"\"\n",
        "    Creates a matrix out of psi and multiply it against its inverse, \n",
        "    resulting in a column vector in the form [[alfa]. [beta]].\n",
        "    Does the operation |psi><psi| from Equation #18 or #19 in the Article.\n",
        "  \"\"\"\n",
        "  psi = np.matrix(psi)\n",
        "  return psi * psi.getH()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_and_execute_classifier(vectorX, vectorW):\n",
        "  \"\"\"\n",
        "    Applies the ICQ classifier using only the math behind the Quantum Classifier \n",
        "    described in Interactive Quantum Classifier Inspired by Quantum Open System Theory\n",
        "    article. \n",
        "    After doing so, it gets the result of Equation #20 and returns Z as the predicted class and\n",
        "    the probability of being the class 1.\n",
        "    Works only for binary classifications, therefore, if the probability of class 0 is needed, it can\n",
        "    be 1 - probability of being class 1.\n",
        "  \"\"\"\n",
        "\n",
        "  # Eq #16\n",
        "  sigmaQ = get_sigmaQ(2)\n",
        "\n",
        "  # Eq #17\n",
        "  sigmaE = get_sigmaE(vectorX, vectorW)\n",
        "\n",
        "  # Eq #15\n",
        "  U_operator = get_U_operator(sigmaQ, sigmaE)\n",
        "\n",
        "  # Eq #18 applied on a Quantum state equivalent of Hadamard(|0>) = 1/sqrt(2) * (|0> + |1>) \n",
        "  p_cog = get_p([[1/np.sqrt(2)],[1/np.sqrt(2)]])\n",
        "\n",
        "  # As we must have 1 row per attribute of the input, we need env to be as big as one instance of our input\n",
        "  N = len(vectorX)\n",
        "\n",
        "  # Eq #19 applied on a Quantum state equivalent of Hadamard(|000000...>) = 1/sqrt(N) * (|000000...> + ... + |11111111....>) \n",
        "  p_env = get_p([[1/np.sqrt(N)] for i in range(N)])\n",
        "\n",
        "  # First part of Equation #20 in the Article\n",
        "  quantum_operation = np.array(U_operator * (np.kron(p_cog, p_env)) * U_operator.getH())\n",
        "\n",
        "  # Second part of Equation #20 in the Article\n",
        "  p_cog_new = np.trace(quantum_operation.reshape([2,N,2,N]), axis1=1, axis2=3)\n",
        "\n",
        "  # As the result is a diagonal matrix, the probability of being class 0 will be on position 0,0\n",
        "  p_cog_new_00_2 = p_cog_new[0,0]\n",
        "\n",
        "  # ... and the probability of being class 1 will be on position 1,1\n",
        "  p_cog_new_11_2 = p_cog_new[1,1]\n",
        "  if (p_cog_new_00_2 >= p_cog_new_11_2):\n",
        "    z = 0\n",
        "  else:\n",
        "    z = 1\n",
        "  return z, p_cog_new_11_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_weights(weights, y, z, x, p, n):\n",
        "  \"\"\"\n",
        "    Updates the weights. Equation #34 in the Article.\n",
        "    \n",
        "    y is the expected classification [0, 1];\n",
        "    z is the actual classification [0, 1];\n",
        "    x is the attribute vector;\n",
        "    p is the probability of the class 1 (0, 1);\n",
        "    n is the learning rate.\n",
        "  \"\"\"\n",
        "  # Eq 33\n",
        "  loss_derivative_on_weight = (1-(p**2))*x\n",
        "\n",
        "  # Eq 34\n",
        "  weights = weights-n*(z-y)*loss_derivative_on_weight\n",
        "  return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_and_execute_classifiers(vectorX, vectorWs):\n",
        "  \"\"\"\n",
        "    Creates classifiers with differents weights and outputs the index of\n",
        "    the weight that has the highest probability of having class 1\n",
        "  \"\"\"\n",
        "  list_p11_i = []\n",
        "\n",
        "  for vectorWs_i in vectorWs:\n",
        "    # First we create and execute the classifier\n",
        "    zi, p11_i = create_and_execute_classifier(vectorX, vectorWs_i)  \n",
        "\n",
        "    # Then we save the probability of being class 1\n",
        "    list_p11_i.append(p11_i)\n",
        "  \n",
        "  # Finally, we get the biggest prob\n",
        "  return np.argmax(list_p11_i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_and_execute_1_classifier(X, Y, w, n=0.1):\n",
        "  \"\"\"\"\n",
        "    Creates, train and executes 1 classifier throughout all instances,\n",
        "    updating the weights instance per instance (batch-size = 1).\n",
        "   \n",
        "    X is a NxM vector of Atributes\n",
        "    Y is the N vector of Classes\n",
        "    W is the M vector of Weights\n",
        "    N is the learning rate\n",
        "    \n",
        "    Returns updated_weight, error\n",
        "  \"\"\"\n",
        "  error = 0\n",
        "  \n",
        "  for x,y in zip(X,Y):\n",
        "    # First we create and execute the classifier\n",
        "    z, p11 = create_and_execute_classifier(x,w)\n",
        "\n",
        "    # Then we update our weights based on our result\n",
        "    w = update_weights(w,y,z,x,p11,n)\n",
        "\n",
        "    # Next we store the error\n",
        "    if (z != y):\n",
        "      error += 1\n",
        "\n",
        "  return w, error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def training_1_batch_classifier(X, Y, w, n=0.1):\n",
        "  \"\"\"\n",
        "    Creates, trains and executes 1 classifier after all instances using the average\n",
        "    of the input vector (X), class vector (Y), probability of being class 1 and assigned\n",
        "    class (uses updates_weights(w,y_avg,z_avg,x_avg,p11_avg,n)).\n",
        "    \n",
        "    X is a NxM vector of Atributes\n",
        "    Y is the N vector of Classes\n",
        "    W is the M vector of Weights\n",
        "    N is the learning rate\n",
        "\n",
        "    Returns updated_weight, error\n",
        "  \"\"\"\n",
        "  error = 0\n",
        "  x_avg = 0\n",
        "  y_avg = 0\n",
        "  z_avg = 0\n",
        "  p11_avg = 0\n",
        "  lines = X.shape[0]\n",
        "\n",
        "  # We do something similar to create_and_execute_1_classifier method, but updating only once\n",
        "  for x,y in zip(X,Y):\n",
        "    z, p11 = create_and_execute_classifier(x,w)\n",
        "    x_avg += x/lines\n",
        "    y_avg += y/lines\n",
        "    z_avg += z/lines\n",
        "    p11_avg += p11/lines\n",
        "    if (z != y):\n",
        "      error += 1\n",
        "\n",
        "  w = update_weights(w,y_avg,z_avg,x_avg,p11_avg,n)\n",
        "\n",
        "  return w, error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def training_n_steps_classifier(X, Y, w, batch_size, Nsteps, n=0.1, stop_error_zero=False):\n",
        "  \"\"\"\n",
        "    Creates, trains and executes classifiers through batches using @training_1_batch_classifier method.\n",
        "    \n",
        "    X is a NxM vector of Atributes\n",
        "    Y is the N vector of Classes\n",
        "    w is the M vector of Weights\n",
        "    batch_size is the number of instances used per batch training\n",
        "    Nsteps is the number of times the classifier will be training. Similar to number of epochs\n",
        "    n is the learning rate\n",
        "    stop_error_zero defines whether we should stop when we have error equals zero\n",
        "\n",
        "    Returns the weights after the training and the min error obtained after executing\n",
        "  \"\"\"\n",
        "  # First we need to know how many batches we will have for training\n",
        "  lines = X.shape[0]\n",
        "  splits = math.ceil(lines / batch_size)\n",
        "  \n",
        "  min_error = np.Inf\n",
        "  min_w_error = np.Inf\n",
        "\n",
        "  for i in range(Nsteps):\n",
        "    errors = 0\n",
        "    # For each batch split, we need to train our classfier\n",
        "    for split in range(splits):\n",
        "      # First step is to define our dataset\n",
        "      X_batch = X[split*batch_size:(split+1)*batch_size , :]\n",
        "      Y_batch = Y[split*batch_size:(split+1)*batch_size]\n",
        "\n",
        "      # We save the weights in which we're executing the current classifier in case this is the best one\n",
        "      w_old = w[:]\n",
        "\n",
        "      # Then we train and classify for this part of the dataset\n",
        "      w, error = training_1_batch_classifier(X_batch,Y_batch,w=w,n=n)\n",
        "      errors += error\n",
        "    \n",
        "    # and save the sum of errors for all batches if needed \n",
        "    if (min_error > errors):\n",
        "      min_w_error = w_old\n",
        "      min_error = errors\n",
        "      \n",
        "    if (errors == 0 and stop_error_zero):\n",
        "      break\n",
        "  return min_w_error, min_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def replicate_classes(data_set, features_names, classes):\n",
        "  \"\"\"\n",
        "    Creates datasets which assigns 1 to one class and 0 to another, and balance\n",
        "    the number of instances of class 0 and class 1.\n",
        "    \n",
        "    Returns an array with all datasets. For Iris dataset\n",
        "    list_x_y[0] = datasets where all instances of class 0 has class 1 and instances of other classes has class 0\n",
        "    list_x_y[1] = datasets where all instances of class 1 has class 1 and instances of other classes has class 0\n",
        "    list_x_y[2] = datasets where all instances of class 2 has class 1 and instances of other classes has class 0\n",
        "    \n",
        "    It also duplicates the instances with the targeted class, making all datasets having 100 instances of class 0\n",
        "    and 100 instances of class 1 for Iris dataset.\n",
        "  \"\"\"\n",
        "  list_x_y = []\n",
        "  n_classes = len(classes)\n",
        "\n",
        "  for class_i in classes:\n",
        "    # We don't want to change the original dataset, so we make a copy of it\n",
        "    y_class_i = data_set.copy()\n",
        "\n",
        "    # First we change all classes to n + 1, in order for it to have a special value\n",
        "    y_class_i.loc[ y_class_i[\"target\"] == class_i  , \"target\"]  = n_classes+1\n",
        "\n",
        "    # Then, we change every class different from the one we want to 0\n",
        "    y_class_i.loc[ y_class_i[\"target\"] < n_classes  , \"target\"]  = 0\n",
        "\n",
        "    # Finally, we change every class that is equal to the one we want to 1\n",
        "    y_class_i.loc[ y_class_i[\"target\"] == (n_classes+1)  , \"target\"]  = 1\n",
        "\n",
        "    # Last but not least, we replicate the dataset to have a balanced number of instances with\n",
        "    # desired and undesired classes. As we're dealing only with Iris, which has 50/50/50, we only\n",
        "    # need to replicate once\n",
        "    y_class_i = pd.concat([y_class_i, y_class_i[y_class_i[\"target\"]==1]], axis=0)\n",
        "  \n",
        "    # Now we split between attributes and target\n",
        "    list_x_y.append([y_class_i[features_names], y_class_i[\"target\"]])\n",
        "\n",
        "  return list_x_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_many_classifiers(X, y, list_of_weights_classifiers):\n",
        "  \"\"\"\n",
        "    Uses the classifier weights to try to predict the correct class and then prints the metrics\n",
        "\n",
        "    Returns precision, recall, f1_measure, accuracy, y_pred\n",
        "  \"\"\"\n",
        "  lines = X.shape[0]\n",
        "  y_pred = []\n",
        "\n",
        "  # For each instance, we try to predict the correct class\n",
        "  for i in range(lines):\n",
        "    output_class = create_and_execute_classifiers(X.iloc[i, :].values, list_of_weights_classifiers)\n",
        "    y_pred.append(output_class)\n",
        "  \n",
        "  # Then we get the correct classes to start recording\n",
        "  y_true = y[\"target\"].tolist()\n",
        "\n",
        "  # To do so, we count the number of hits\n",
        "  hit=0\n",
        "  for y, yhat in zip(y_true, y_pred):\n",
        "    if (y == yhat):\n",
        "      hit+=1\n",
        "  #print(\"Acertos: \", hit)\n",
        "\n",
        "  # Finally, we calculate every score we need\n",
        "  precision = precision_score(y_true, y_pred, labels=[0,1,2], average='micro')\n",
        "  recall = recall_score(y_true, y_pred, labels=[0,1,2],average='micro')\n",
        "  f1_measure = f1_score(y_true, y_pred, labels=[0,1,2], average='micro')\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "  #print(\"Precision:\", precision, \", Recall:\", recall, \", F1-Score:\", f1_measure)\n",
        "\n",
        "  return precision, recall, f1_measure, accuracy, y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def avg_metrics(measures):\n",
        "  \"\"\" \n",
        "    Prints the average of the execution portraid in measures.\n",
        "    The \"measures\" param must be a vector of the following tuple:\n",
        "    (precision, recall, f1_measure, accuracy)\n",
        "  \"\"\"\n",
        "  precisions = 0\n",
        "  recalls = 0\n",
        "  f1_measures = 0\n",
        "  accs = 0\n",
        "  count = 0\n",
        "\n",
        "  for precision, recall, f1_measure, accuracy in measures:\n",
        "    precisions += precision\n",
        "    recalls += recall\n",
        "    f1_measures += f1_measure\n",
        "    accs += accuracy\n",
        "    count+=1\n",
        "    \n",
        "  print(\"acc\", accs/count)\n",
        "  print(\"precision\", precisions/count)\n",
        "  print(\"recall\", recalls/count)\n",
        "  print(\"f1_measure\", f1_measures/count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def standard_scaling(df):\n",
        "    \"\"\"\n",
        "        Uses the scikit-learn StandardScaler to normalize the dataset. \n",
        "        See https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "    \"\"\"\n",
        "    df_scaled = df.copy()\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(df_scaled)\n",
        "    df_scaled = scaler.transform(df_scaled)\n",
        "    return df_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def maximum_absolute_scaling_by_column(df):\n",
        "    \"\"\"\n",
        "        Divides the whole column by the max absolute value available.\n",
        "    \"\"\"\n",
        "    # copy the dataframe\n",
        "    df_scaled = df.copy()\n",
        "    \n",
        "    # apply maximum absolute scaling\n",
        "    for column in df_scaled.columns:\n",
        "        df_scaled[column] = df_scaled[column]  / df_scaled[column].abs().max()\n",
        "    return df_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def min_max_scaling_by_column(df):\n",
        "    \"\"\"\n",
        "        Divides the whole column by the difference between its max and min values.\n",
        "    \"\"\"\n",
        "    # copy the dataframe\n",
        "    df_norm = df.copy()\n",
        "    \n",
        "    # apply min-max scaling\n",
        "    for column in df_norm.columns:\n",
        "        df_norm[column] = (df_norm[column] - df_norm[column].min()) / (df_norm[column].max() - df_norm[column].min())\n",
        "        \n",
        "    return df_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hjrH_WOzje1G"
      },
      "outputs": [],
      "source": [
        "def min_max_scaling_by_column_type_2(df):\n",
        "    \"\"\"\n",
        "        Divides the whole column through the following equation:\n",
        "        column = (value - min) / (max - min) - 1\n",
        "    \"\"\"\n",
        "    # copy the dataframe\n",
        "    a, b= -1, 0\n",
        "    df_norm = df.copy()\n",
        "    \n",
        "    # apply min-max scaling\n",
        "    for column in df_norm.columns:\n",
        "        df_norm[column] =(b-a)*(df_norm[column] - df_norm[column].min()) / (df_norm[column].max() - df_norm[column].min())+ a\n",
        "        \n",
        "    return df_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def training_k_fold_classifier(kfold, X, y, Nsteps, batch_size, n_learning_rate, features_names, classes, normalizing_function=min_max_scaling_by_column_type_2, random_state=42):\n",
        "    # Instantiating the K-Fold cross validation object with 5 folds\n",
        "    k_folds = StratifiedKFold(n_splits = kfold, shuffle = True, random_state = random_state)\n",
        "    metrics = []\n",
        "\n",
        "    # Iterating through each of the folds in K-Fold\n",
        "    for train_index, test_index in k_folds.split(X, y):\n",
        "      # print(\"TRAIN INDEX\", train_index, \"VAL_INDEX\", val_index)\n",
        "\n",
        "      # Splitting the training set from the validation set for this specific fold\n",
        "      X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
        "      y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "      # Applies the normalizing_function on each Dataset\n",
        "      X_train = normalizing_function(X_train)\n",
        "      X_test = normalizing_function(X_test)\n",
        "      \n",
        "      # We create a new Pandas dataset\n",
        "      dataset = pd.concat([X_train, y_train], axis=1)\n",
        "      \n",
        "      #print(\"DATASET\", dataset)\n",
        "      #X1, y1, X2, y2, X3, y3 \n",
        "\n",
        "      # This list contains 3 values, each one being a dataset for each class\n",
        "      # of the iris dataset. See @replicate_classes doc\n",
        "      list_of_x_y = replicate_classes(dataset, features_names, classes)\n",
        "      list_of_trainned_w = []\n",
        "      count=0\n",
        "\n",
        "      # Then, for each dataset of each class, we \n",
        "      for Xi, yi in list_of_x_y:\n",
        "        # We initialize the weights vector with values 0.1. \n",
        "        # The number of rows must be the number of attributes, as it is a diagonal matrix\n",
        "        w = [0.1 for i in range(Xi.shape[1])]\n",
        "\n",
        "        # Now we need to convert our datasets into np arrays, as it's the type expected in the training_n_steps_classifier\n",
        "        X_train = np.array(Xi)\n",
        "        y_train = np.array(yi)\n",
        "\n",
        "        # Now we update our weights and save the error for printing\n",
        "        wi, error = training_n_steps_classifier(X_train,y_train, w, Nsteps=Nsteps, batch_size=batch_size, n=n_learning_rate, stop_error_zero=True)\n",
        "        print(\"error #\", count, error)\n",
        "\n",
        "        list_of_trainned_w.append(wi)\n",
        "        count+=1\n",
        "\n",
        "      # For testing, we try every weights we already had before, in order to find the best one\n",
        "      precision, recall, f1_measure, accuracy, y_pred = test_many_classifiers(X_test , y_test, list_of_trainned_w)\n",
        "\n",
        "      # And then we append the metrics for this specific execution\n",
        "      metrics.append([precision, recall, f1_measure, accuracy])\n",
        "      print(\"metricas, precision, recall, f1_measure, acc\", metrics[-1])\n",
        "      #print(\"Confusion Matrix #\", count, confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    return list_of_trainned_w, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZCncDOxkbWG",
        "outputId": "5606c945-63ee-4286-a341-a2bfac3d2553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "error # 0 16\n",
            "error # 1 47\n",
            "error # 2 16\n",
            "metricas, precision, recall, f1_measure, acc [0.8666666666666667, 0.8666666666666667, 0.8666666666666667, 0.8666666666666667]\n",
            "error # 0 18\n",
            "error # 1 45\n",
            "error # 2 17\n",
            "metricas, precision, recall, f1_measure, acc [0.8, 0.8, 0.8000000000000002, 0.8]\n",
            "error # 0 16\n"
          ]
        }
      ],
      "source": [
        "# Now that we have built all methods, we can have our training\n",
        "\n",
        "# First thing is to load the iris dataset from sklearn\n",
        "iris = datasets.load_iris(as_frame=True)\n",
        "\n",
        "# Then we get our X and Y\n",
        "X_iris = pd.DataFrame(data= iris['data'],\n",
        "                      columns= iris['feature_names'])\n",
        "y_iris = pd.DataFrame(data= iris['target'],\n",
        "                      columns= ['target'])\n",
        "\n",
        "# And then execute it\n",
        "list_of_trainned_w, metrics = training_k_fold_classifier(kfold=10, \n",
        "                                                            X=X_iris, \n",
        "                                                            y=y_iris, \n",
        "                                                            Nsteps=2000, \n",
        "                                                            batch_size=60, \n",
        "                                                            n_learning_rate=0.009,\n",
        "                                                            features_names=iris['feature_names'], \n",
        "                                                            classes=[0,1,2],\n",
        "                                                            normalizing_function=min_max_scaling_by_column_type_2,\n",
        "                                                            random_state=42) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rThlHhe9Btu",
        "outputId": "16b0e8cd-c8ff-4a1c-8ca2-0b2e5be9391a"
      },
      "outputs": [],
      "source": [
        "# Most important part - what are our metrics?\n",
        "avg_metrics(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now that we have built all methods, we can have our training\n",
        "\n",
        "# First thing is to load the iris dataset from sklearn\n",
        "iris = datasets.load_iris(as_frame=True)\n",
        "\n",
        "# Then we get our X and Y\n",
        "X_iris = pd.DataFrame(data= iris['data'],\n",
        "                      columns= iris['feature_names'])\n",
        "y_iris = pd.DataFrame(data= iris['target'],\n",
        "                      columns= ['target'])\n",
        "\n",
        "# And then execute it\n",
        "list_of_trainned_w, metrics = training_k_fold_classifier(kfold=10, \n",
        "                                                            X=X_iris, \n",
        "                                                            y=y_iris, \n",
        "                                                            Nsteps=2000, \n",
        "                                                            batch_size=60, \n",
        "                                                            n_learning_rate=0.009,\n",
        "                                                            features_names=iris['feature_names'], \n",
        "                                                            classes=[0,1,2],\n",
        "                                                            normalizing_function=min_max_scaling_by_column,\n",
        "                                                            random_state=42) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Most important part - what are our metrics?\n",
        "avg_metrics(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now that we have built all methods, we can have our training\n",
        "\n",
        "# First thing is to load the iris dataset from sklearn\n",
        "iris = datasets.load_iris(as_frame=True)\n",
        "\n",
        "# Then we get our X and Y\n",
        "X_iris = pd.DataFrame(data= iris['data'],\n",
        "                      columns= iris['feature_names'])\n",
        "y_iris = pd.DataFrame(data= iris['target'],\n",
        "                      columns= ['target'])\n",
        "\n",
        "# And then execute it\n",
        "list_of_trainned_w, metrics = training_k_fold_classifier(kfold=10, \n",
        "                                                            X=X_iris, \n",
        "                                                            y=y_iris, \n",
        "                                                            Nsteps=2000, \n",
        "                                                            batch_size=60, \n",
        "                                                            n_learning_rate=0.009,\n",
        "                                                            features_names=iris['feature_names'], \n",
        "                                                            classes=[0,1,2],\n",
        "                                                            normalizing_function=maximum_absolute_scaling_by_column,\n",
        "                                                            random_state=42) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Most important part - what are our metrics?\n",
        "avg_metrics(metrics)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
